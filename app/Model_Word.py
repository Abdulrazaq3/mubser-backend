# Model_Word.py
"""
ØªÙ†Ø¨Ø¤ ÙƒÙ„Ù…Ø§Øª (89 ØµÙ†Ù) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime + MediaPipe Holistic
- Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: Mubser_model_89cls_64.onnx
- Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§: Mubser_model_89cls_64.meta.json
- Ø¥Ø¯Ø®Ø§Ù„: ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© (PIL.Image Ø£Ùˆ Ù…Ø³Ø§Ø±)
- Ø¥Ø®Ø±Ø§Ø¬: (label, confidence, top_k_list)

âœ… ØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Lazy Loading Ù„ØªØ³Ø±ÙŠØ¹ startup Ø§Ù„Ø³ÙŠØ±ÙØ±
"""

from typing import Tuple, List, Optional, Union
from pathlib import Path
import json
import logging
import atexit

import numpy as np
from PIL import Image

# ================= Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª =================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Model_Word")

# ================= Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª =================
MODEL_PATH = Path("Mubser_model_89cls_64.onnx")
META_PATH  = Path("Mubser_model_89cls_64.meta.json")

# ================= Ù…ØªØºÙŠØ±Ø§Øª Lazy Loading =================
_meta_loaded = False
_session_loaded = False
_holistic_loaded = False

CLASSES: List[str] = []
IMG_SIZE: int = 64
NORM_MEAN: Optional[List[float]] = None
NORM_STD: Optional[List[float]] = None
LABEL_MAPPING: Optional[dict] = None

_session = None
_input_name = None
_output_name = None
_holistic = None
mp_holistic = None


def _load_metadata():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©"""
    global _meta_loaded, CLASSES, IMG_SIZE, NORM_MEAN, NORM_STD, LABEL_MAPPING
    
    if _meta_loaded:
        return True
    
    try:
        logger.info("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§...")
        with META_PATH.open("r", encoding="utf-8") as f:
            _meta = json.load(f)
        CLASSES = _meta["classes"]
        IMG_SIZE = int(_meta.get("img_size", 64))
        _norm = _meta.get("normalize") or {}
        NORM_MEAN = _norm.get("mean")
        NORM_STD = _norm.get("std")
        LABEL_MAPPING = _meta.get("mapping")
        logger.info(f"âœ… Loaded meta: {len(CLASSES)} classes, img_size={IMG_SIZE}")
        _meta_loaded = True
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to load meta '{META_PATH}': {e}")
        return False


def _load_onnx_session():
    """ØªØ­Ù…ÙŠÙ„ ONNX Session Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©"""
    global _session_loaded, _session, _input_name, _output_name
    
    if _session_loaded:
        return True
    
    try:
        logger.info("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ONNX...")
        import onnxruntime as ort
        _session = ort.InferenceSession(str(MODEL_PATH), providers=["CPUExecutionProvider"])
        _input_name = _session.get_inputs()[0].name
        _output_name = _session.get_outputs()[0].name
        logger.info("âœ… ONNX session initialized (CPUExecutionProvider)")
        _session_loaded = True
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to initialize ONNX session: {e}")
        return False


def _load_holistic():
    """ØªØ­Ù…ÙŠÙ„ MediaPipe Holistic Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©"""
    global _holistic_loaded, _holistic, mp_holistic
    
    if _holistic_loaded:
        return _holistic is not None
    
    try:
        logger.info("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ MediaPipe Holistic...")
        import mediapipe as mp
        if hasattr(mp, 'solutions'):
            mp_holistic = mp.solutions.holistic
            _holistic = mp_holistic.Holistic(
                static_image_mode=True,
                model_complexity=1,
                refine_face_landmarks=False,
                min_detection_confidence=0.6
            )
            logger.info("âœ… MediaPipe Holistic initialized successfully")
            _holistic_loaded = True
            return True
        else:
            logger.warning("âš ï¸ MediaPipe solutions not found")
            _holistic_loaded = True
            return False
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to initialize MediaPipe: {e}")
        _holistic_loaded = True
        return False


def _close_holistic():
    try:
        if _holistic:
            _holistic.close()
    except Exception:
        pass

atexit.register(_close_holistic)


def _ensure_loaded():
    """Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª"""
    if not _load_metadata():
        raise RuntimeError("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§")
    if not _load_onnx_session():
        raise RuntimeError("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ONNX")
    _load_holistic()  # Ø§Ø®ØªÙŠØ§Ø±ÙŠ - Ù„Ø§ Ù†ÙØ´Ù„ Ø¥Ø°Ø§ ÙØ´Ù„


# =====================================================
#                ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©/Ø§Ù„ØªÙ†Ø¨Ø¤
# =====================================================

def crop_holistic_union_pil(img_pil: Image.Image, pad: int = 20, max_size: int = 640) -> Image.Image:
    """
    ÙŠÙ‚ØµÙ‘ Ù…Ø³ØªØ·ÙŠÙ„Ø§Ù‹ ÙˆØ§Ø­Ø¯Ù‹Ø§ ÙŠØ¶Ù… Ø§Ù„ÙŠØ¯ÙŠÙ† + Ø§Ù„ÙˆØ¬Ù‡ + Ø§Ù„Ø¬Ø³Ù…
    """
    import cv2
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ù…ÙŠÙ„ MediaPipe
    _load_holistic()
    
    if _holistic is None:
        # Fallback Ù…Ø¨Ø§Ø´Ø± Ù„Ù„Ù‚Øµ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¹Ù…Ù„ MediaPipe
        bgr = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
        h, w = bgr.shape[:2]
        center_x, center_y = w // 2, h // 2
        crop_size = min(w, h) * 3 // 4
        x1 = max(0, center_x - crop_size // 2)
        y1 = max(0, center_y - crop_size // 2)
        x2 = min(w, x1 + crop_size)
        y2 = min(h, y1 + crop_size)
        crop = bgr[y1:y2, x1:x2]
        rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
        return Image.fromarray(rgb)

    try:
        # ØªØµØºÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªØ³Ø±ÙŠØ¹ MediaPipe
        w, h = img_pil.size
        if max(w, h) > max_size:
            scale = max_size / max(w, h)
            new_w, new_h = int(w * scale), int(h * scale)
            img_pil = img_pil.resize((new_w, new_h), Image.LANCZOS)
        
        bgr = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
        h, w = bgr.shape[:2]
        res = _holistic.process(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))

        xs, ys = [], []

        # Ø§Ù„ÙŠØ¯Ø§Ù†
        for hand_lms in [res.left_hand_landmarks, res.right_hand_landmarks]:
            if hand_lms:
                for lm in hand_lms.landmark:
                    xs.append(int(lm.x * w))
                    ys.append(int(lm.y * h))

        # Ø§Ù„ÙˆØ¬Ù‡ (Ù†Ù‚Ø§Ø· Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© ÙÙ‚Ø·)
        if res.face_landmarks:
            key_face_points = [10, 152, 234, 454, 1]
            lms = res.face_landmarks.landmark
            for k in key_face_points:
                if 0 <= k < len(lms):
                    lm = lms[k]
                    xs.append(int(lm.x * w))
                    ys.append(int(lm.y * h))

        # Ø§Ù„Ø¬Ø³Ù… (Ù†Ù‚Ø§Ø· Ø£Ø³Ø§Ø³ÙŠØ©)
        if res.pose_landmarks:
            pose_ids = [0, 11, 12, 23, 24]  # Ø±Ø£Ø³ØŒ ÙƒØªÙÙŠÙ†ØŒ ÙˆØ±ÙƒÙŠÙ†
            lms = res.pose_landmarks.landmark
            for k in pose_ids:
                if 0 <= k < len(lms):
                    lm = lms[k]
                    xs.append(int(lm.x * w))
                    ys.append(int(lm.y * h))

        if not xs:
            logger.warning("âš ï¸ Ù„Ù… ÙŠÙÙƒØªØ´Ù Ø¥Ù†Ø³Ø§Ù† - Ø§Ø³ØªØ®Ø¯Ø§Ù… crop Ù…Ø±ÙƒØ²ÙŠ")
            # âœ… fallback: Ù‚Øµ Ù…Ø±ÙƒØ²ÙŠ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
            center_x, center_y = w // 2, h // 2
            crop_size = min(w, h) * 3 // 4
            x1 = max(0, center_x - crop_size // 2)
            y1 = max(0, center_y - crop_size // 2)
            x2 = min(w, x1 + crop_size)
            y2 = min(h, y1 + crop_size)
            crop = bgr[y1:y2, x1:x2]
            rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
            return Image.fromarray(rgb)

        x1, x2 = max(0, min(xs) - pad), min(w, max(xs) + pad)
        y1, y2 = max(0, min(ys) - pad), min(h, max(ys) + pad)
        
        if x2 <= x1 or y2 <= y1:
            logger.warning("âš ï¸ ØµÙ†Ø¯ÙˆÙ‚ ØºÙŠØ± ØµØ§Ù„Ø­")
            return img_pil

        crop = bgr[y1:y2, x1:x2]
        rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
        return Image.fromarray(rgb)
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù‚Øµ: {e}")
        # fallback Ø¢Ù…Ù†
        return img_pil.resize((IMG_SIZE, IMG_SIZE))


def _apply_optional_normalize(x: np.ndarray) -> np.ndarray:
    """
    ÙŠØ·Ø¨Ù‘Ù‚ Normalize (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ø¥Ø°Ø§ ØªÙ… ØªØ¹Ø±ÙŠÙÙ‡ ÙÙŠ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§
    - x Ø´ÙƒÙ„Ù‡Ø§ (1,1,H,W) ÙˆÙ‚ÙŠÙ…Ù‡Ø§ [0..1]
    """
    if NORM_MEAN and NORM_STD and len(NORM_MEAN) >= 1 and len(NORM_STD) >= 1:
        mean = float(NORM_MEAN[0])
        std = float(NORM_STD[0]) if float(NORM_STD[0]) != 0 else 1.0
        x = (x - mean) / std
    return x


def preprocess_pil(img_pil: Image.Image, enhance: bool = True) -> np.ndarray:
    """
    pipeline Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù‘Ù†
    """
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§
    _load_metadata()
    
    # 1. Ù‚Øµ
    img_pil = crop_holistic_union_pil(img_pil, pad=20)

    # 2. âœ… ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    if enhance:
        from PIL import ImageEnhance
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ù‚Ù„ÙŠÙ„Ø§Ù‹
        enhancer = ImageEnhance.Contrast(img_pil)
        img_pil = enhancer.enhance(1.2)
        # Ø¶Ø¨Ø· Ø§Ù„Ø³Ø·ÙˆØ¹
        enhancer = ImageEnhance.Brightness(img_pil)
        img_pil = enhancer.enhance(1.1)

    # 3. Ø±Ù…Ø§Ø¯ÙŠ + ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù…
    img_pil = img_pil.convert("L").resize((IMG_SIZE, IMG_SIZE), Image.LANCZOS)

    # 4. Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ©
    x = np.array(img_pil, dtype=np.float32) / 255.0

    # 5. âœ… Histogram Equalization (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù„Ø¥Ø¶Ø§Ø¡Ø© Ø§Ù„Ø³ÙŠØ¦Ø©)
    # x = cv2.equalizeHist((x * 255).astype(np.uint8)) / 255.0

    # 6. Normalize Ù…Ù† Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§
    x = _apply_optional_normalize(x)

    # 7. Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„
    x = x[None, None, :, :]
    return x


def _softmax(z: np.ndarray) -> np.ndarray:
    z = z - np.max(z)
    ez = np.exp(z)
    return ez / np.sum(ez)


def predict_word_from_pil(img_pil: Image.Image, top_k: int = 5) -> Tuple[str, float, List[Tuple[str, float]]]:
    """
    ØªÙ†Ø¨Ø¤ Ù…Ù† PIL.Image
    ÙŠØ±Ø¬Ù‘Ø¹: (label, confidence, top_k_list)
    """
    # âœ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª (Lazy Loading)
    _ensure_loaded()
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
    x = preprocess_pil(img_pil)

    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    outputs = _session.run([_output_name], {_input_name: x})
    logits = outputs[0][0]  # (num_classes,)

    probs = _softmax(logits)
    top_idx = int(np.argmax(probs))
    top_label = CLASSES[top_idx]
    top_conf = float(probs[top_idx])

    # Ø£Ø¹Ù„Ù‰ k
    k = int(max(1, min(top_k, len(CLASSES))))
    top_k_indices = np.argsort(probs)[-k:][::-1]
    top_k_list = [(CLASSES[i], float(probs[i])) for i in top_k_indices]

    # Ø¥Ø°Ø§ Ø¹Ù†Ø¯Ù†Ø§ mapping ÙÙŠ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§ØŒ Ù†Ù‚Ø¯Ø± Ù†Ø±ÙÙ‚ Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©
    if LABEL_MAPPING and top_label in LABEL_MAPPING:
        mapped = LABEL_MAPPING[top_label]
        top_label = f"{top_label} | {mapped}"

        # Ù†Ø·Ø¨Ù‘Ù‚ Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡ Ø¹Ù„Ù‰ top_k_list
        new_top = []
        for k_lbl, k_p in top_k_list:
            if k_lbl in LABEL_MAPPING:
                new_top.append((f"{k_lbl} | {LABEL_MAPPING[k_lbl]}", k_p))
            else:
                new_top.append((k_lbl, k_p))
        top_k_list = new_top

    return top_label, top_conf, top_k_list


def check_image_quality(img_pil: Image.Image) -> bool:
    """
    ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    """
    import cv2
    
    w, h = img_pil.size
    
    # 1. Ø­Ø¬Ù… Ø£Ø¯Ù†Ù‰
    if w < 100 or h < 100:
        logger.warning(f"âš ï¸ Ø§Ù„ØµÙˆØ±Ø© ØµØºÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹: {w}x{h}")
        return False
    
    # 2. ÙØ­Øµ Ø§Ù„Ø¶Ø¨Ø§Ø¨ÙŠØ© (Laplacian variance)
    gray = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2GRAY)
    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
    
    if laplacian_var < 50:  # Ø¹ØªØ¨Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ©
        logger.warning(f"âš ï¸ Ø§Ù„ØµÙˆØ±Ø© Ø¶Ø¨Ø§Ø¨ÙŠØ©: variance={laplacian_var:.2f}")
        return False
    
    return True


def predict_word(image: Union[str, Path, Image.Image], top_k: int = 5) -> Tuple[str, float, List[Tuple[str, float]]]:
    """
    ØªÙ†Ø¨Ø¤ Ù…Ø¹ ÙØ­Øµ Ø§Ù„Ø¬ÙˆØ¯Ø©
    """
    if isinstance(image, (str, Path)):
        img_pil = Image.open(image).convert("RGB")
    elif isinstance(image, Image.Image):
        img_pil = image
    else:
        raise TypeError("image ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ø³Ø§Ø±Ø§Ù‹ Ø£Ùˆ PIL.Image")
    
    # âœ… ÙØ­Øµ Ø§Ù„Ø¬ÙˆØ¯Ø©
    if not check_image_quality(img_pil):
        logger.warning("âš ï¸ Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø© Ù…Ù†Ø®ÙØ¶Ø© - Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù‚Ø¯ Ù„Ø§ ØªÙƒÙˆÙ† Ø¯Ù‚ÙŠÙ‚Ø©")
    
    return predict_word_from_pil(img_pil, top_k=top_k)


def predict_word_with_tta(
    img_pil: Image.Image, 
    top_k: int = 5,
    use_tta: bool = False
) -> Tuple[str, float, List[Tuple[str, float]]]:
    """
    ØªÙ†Ø¨Ø¤ Ù…Ø¹ TTA (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    """
    if not use_tta:
        return predict_word_from_pil(img_pil, top_k)
    
    # âœ… Test Time Augmentation
    predictions = []
    
    # 1. Ø§Ù„Ø£ØµÙ„ÙŠØ©
    predictions.append(predict_word_from_pil(img_pil, top_k))
    
    # 2. Ø§Ù†Ø¹ÙƒØ§Ø³ Ø£ÙÙ‚ÙŠ
    flipped = img_pil.transpose(Image.FLIP_LEFT_RIGHT)
    predictions.append(predict_word_from_pil(flipped, top_k))
    
    # 3. Ø¯ÙˆØ±Ø§Ù† Ø·ÙÙŠÙ
    for angle in [-5, 5]:
        rotated = img_pil.rotate(angle, fillcolor='white')
        predictions.append(predict_word_from_pil(rotated, top_k))
    
    # âœ… Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (voting)
    label_votes = {}
    for label, conf, _ in predictions:
        label_votes[label] = label_votes.get(label, 0) + conf
    
    # Ø£Ø¹Ù„Ù‰ ØªØµÙˆÙŠØª
    best_label = max(label_votes, key=label_votes.get)
    avg_conf = label_votes[best_label] / len(predictions)
    
    # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù€ top_k
    sorted_labels = sorted(label_votes.items(), key=lambda x: x[1], reverse=True)[:top_k]
    top_k_list = [(lbl, conf / len(predictions)) for lbl, conf in sorted_labels]
    
    return best_label, avg_conf, top_k_list


# Ø§Ø®ØªÙŠØ§Ø±ÙŠ: Ø¯Ø§Ù„Ø© Ø¨Ø³ÙŠØ·Ø© ØªÙØ±Ø¬Ø¹ Ù†ØµØ§Ù‹ ÙÙ‚Ø· (Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø§Øª)
def dummy_extract_text(image: Union[str, Path, Image.Image]) -> str:
    try:
        label, conf, _ = predict_word(image, top_k=5)
        return f"{label} ({conf:.2f})"
    except Exception as e:
        return f"prediction_failed: {e}"
