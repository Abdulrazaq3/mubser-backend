"""
Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime Ùˆ MediaPipe

âœ… ØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Lazy Loading Ù„ØªØ³Ø±ÙŠØ¹ startup Ø§Ù„Ø³ÙŠØ±ÙØ±
   - cv2, mediapipe, onnxruntime ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© ÙÙ‚Ø·
"""
import json
from pathlib import Path
from typing import Tuple, Optional, List
import logging

import numpy as np
from PIL import Image
# âœ… ØªØ£Ø®ÙŠØ± imports Ø§Ù„Ø«Ù‚ÙŠÙ„Ø© - Ø³ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ø¯Ø§Ø®Ù„ SignLanguagePredictor

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SignLanguagePredictor:
    """
    ÙƒÙ„Ø§Ø³ Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„ØµÙˆØ±
    """
    
    def __init__(
        self,
        model_path: str = "Mubser_model.onnx",
        metadata_path: str = "Mubser_model.meta.json",
        providers: List[str] = None
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªÙ†Ø¨Ø¦
        """
        # âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© ÙÙ‚Ø·
        import onnxruntime as ort
        import mediapipe as mp
        self._cv2 = None  # Ø³ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
        self._mp = mp
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§ + Ø§Ù„Ù…Ø§Ø¨Ù†Ù‚
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
                self.classes = metadata["classes"]
                self.img_size = metadata["img_size"]
                self.mapping = metadata.get("mapping", {})  # âœ… Ù‡Ù†Ø§ Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙÙ‚Ø·
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.classes)} ØµÙ†Ù Ù…Ù† Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§: {e}")
            raise
        
        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ONNX
        try:
            if providers is None:
                providers = ["CPUExecutionProvider"]
            
            self.session = ort.InferenceSession(model_path, providers=providers)
            self.input_name = self.session.get_inputs()[0].name
            self.output_names = [o.name for o in self.session.get_outputs()]
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: {providers}")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            raise
        
        # ØªÙ‡ÙŠØ¦Ø© MediaPipe (Fail-safe)
        self.mp_hands = None
        try:
            if hasattr(mp, 'solutions'):
                self.mp_hands = mp.solutions.hands.Hands(
                    static_image_mode=True,
                    max_num_hands=1,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
                logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© MediaPipe")
            else:
                logger.warning("âš ï¸ MediaPipe solutions ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ (ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆØ§ÙÙ‚ Ø¥ØµØ¯Ø§Ø± Python)")
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© MediaPipe (Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Øµ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ): {e}")
    
    def _get_cv2(self):
        """ØªØ­Ù…ÙŠÙ„ cv2 Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©"""
        if self._cv2 is None:
            import cv2
            self._cv2 = cv2
        return self._cv2
    
    def detect_hand_box(self, bgr: np.ndarray, pad: int = 20) -> Optional[Tuple[int, int, int, int]]:
        cv2 = self._get_cv2()
        h, w = bgr.shape[:2]
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
        
        try:
            if self.mp_hands is None:
                return None
                
            results = self.mp_hands.process(rgb)
            if not results.multi_hand_landmarks:
                return None
            
            landmarks = results.multi_hand_landmarks[0].landmark
            xs = [int(lm.x * w) for lm in landmarks]
            ys = [int(lm.y * h) for lm in landmarks]
            
            x1 = max(0, min(xs) - pad)
            y1 = max(0, min(ys) - pad)
            x2 = min(w, max(xs) + pad)
            y2 = min(h, max(ys) + pad)
            
            if x2 <= x1 or y2 <= y1:
                return None
            
            return (x1, y1, x2, y2)
        except:
            return None
    
    def crop_hand(self, img_pil: Image.Image, pad: int = 20) -> Image.Image:
        cv2 = self._get_cv2()
        try:
            bgr = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
            box = self.detect_hand_box(bgr, pad=pad)
            if box is None:
                return img_pil
            
            x1, y1, x2, y2 = box
            crop = bgr[y1:y2, x1:x2]
            if crop.size == 0:
                return img_pil
            
            rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
            return Image.fromarray(rgb)
        except:
            return img_pil
    
    def preprocess(self, img_pil: Image.Image) -> np.ndarray:
        img_pil = self.crop_hand(img_pil, pad=20)
        img_pil = img_pil.convert("L").resize((self.img_size, self.img_size))
        x = np.array(img_pil, dtype=np.float32) / 255.0
        x = x[None, None, :, :]
        return x
    
    def predict(self, image_path: str, top_k: int = 5) -> Tuple[str, float, List[Tuple[str, float]]]:
        img = Image.open(image_path).convert("RGB")
        x = self.preprocess(img)
        outputs = self.session.run(self.output_names, {self.input_name: x})
        logits = outputs[0][0]
        
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / np.sum(exp_logits)
        
        top_idx = int(np.argmax(probs))
        top_label = self.classes[top_idx]
        top_confidence = float(probs[top_idx])
        
        top_k_indices = np.argsort(probs)[-top_k:][::-1]
        top_k_predictions = [(self.classes[idx], float(probs[idx])) for idx in top_k_indices]
        
        return top_label, top_confidence, top_k_predictions
    
    def __del__(self):
        if hasattr(self, 'mp_hands') and self.mp_hands:
            self.mp_hands.close()


# ====== Lazy Loading Pattern ======
_predictor = None
_predictor_loading = False


def get_predictor():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙ†Ø¨Ø¦ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© ÙÙ‚Ø· (Lazy Loading)"""
    global _predictor, _predictor_loading
    
    if _predictor is not None:
        return _predictor
    
    if _predictor_loading:
        # Ù…Ù†Ø¹ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†
        import time
        while _predictor_loading and _predictor is None:
            time.sleep(0.1)
        return _predictor
    
    try:
        _predictor_loading = True
        logger.info("ðŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø±ÙˆÙ (Lazy Loading)...")
        _predictor = SignLanguagePredictor()
        logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø±ÙˆÙ Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªÙ†Ø¨Ø¦: {e}")
        _predictor = None
    finally:
        _predictor_loading = False
    
    return _predictor


# ====== Ø¯ÙˆØ§Ù„ ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ… ======

def predict(image_path: str) -> Tuple[str, float]:
    predictor = get_predictor()
    if predictor is None:
        raise RuntimeError("Ø§Ù„Ù…ØªÙ†Ø¨Ø¦ ØºÙŠØ± Ù…ØªØ§Ø­")
    label, confidence, _ = predictor.predict(image_path)

    # âœ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù„Ø§Ø¨Ù„ Ø¥Ù„Ù‰ Ø¹Ø±Ø¨ÙŠ Ø­Ø³Ø¨ Ù…Ù„Ù Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§
    if hasattr(predictor, "mapping") and predictor.mapping:
        label = predictor.mapping.get(label, label)

    return label, confidence


def dummy_extract_text(image_path: str) -> str:
    try:
        label, confidence = predict(image_path)
        return f"{label} (Ø«Ù‚Ø©: {confidence:.2%})"
    except Exception as e:
        return f"ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ: {str(e)}"


def get_top_predictions(image_path: str, top_k: int = 5) -> List[Tuple[str, float]]:
    predictor = get_predictor()
    if predictor is None:
        raise RuntimeError("Ø§Ù„Ù…ØªÙ†Ø¨Ø¦ ØºÙŠØ± Ù…ØªØ§Ø­")
    _, _, top_k_preds = predictor.predict(image_path, top_k=top_k)
    return top_k_preds

